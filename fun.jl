using Flux, DiffEqFlux, DifferentialEquations, Plots, LinearAlgebra


GDP = [11394358246872.6, 11886411296037.9, 12547852149499.6, 13201781525927, 14081902622923.3, 14866223429278.3, 15728198883149.2, 16421593575529.9, 17437921118338, 18504710349537.1, 19191754995907.1, 20025063402734.2, 21171619915190.4, 22549236163304.4, 22999815176366.2, 23138798276196.2, 24359046058098.6, 25317009721600.9, 26301669369287.8, 27386035164588.8, 27907493159394.4, 28445139283067.1, 28565588996657.6, 29255060755937.6, 30574152048605.8, 31710451102539.4, 32786657119472.8, 34001004119223.5, 35570841010027.7, 36878317437617.5, 37952345258555.4, 38490918890678.7, 39171116855465.5, 39772082901255.8, 40969517920094.4, 42210614326789.4, 43638265675924.6, 45254805649669.6, 46411399944618.2, 47929948653387.3, 50036361141742.2, 51009550274808.6, 52127765360545.5, 53644090247696.9, 55995239099025.6, 58161311618934.2, 60681422072544.7, 63240595965946.1, 64413060738139.7, 63326658023605.9, 66036918504601.7, 68100669928597.9, 69811348331640.1, 71662400667935.7, 73698404958519.1, 75802901433146, 77752106717302.4, 80209237761564.8, 82643194654568.3]
function monomial(dcGDP, cGDP, parameters, t)
    α1, β1, τ1, n, δ = parameters



    dcGDP[1] = α1 * ((cGDP[1])/τ1)^β1
end


function monolinearkoef(dcGDP, cGDP, parameters, t)
    α1, β1, τ1, n, δ, δ2 = parameters



    dcGDP[1] = α1 * ((cGDP[1])/τ1)^(β1*(1 + δ*(t+n) + δ2*((t+n)^2)))
end

function simplemonolinearkoef(dcGDP, cGDP, parameters, t)
    α1, β1, nu, nu1, δ, δ2 = parameters



    dcGDP[1] = α1 * ((cGDP[1]))^(β1*(1
    ##+ δ*(t) + δ2*(t^2)
    ))
end

function linear(dcGDP, cGDP, parameters, t)
    α1, β1, τ1, n, δ = parameters
    dcGDP[1] = τ1*10e10
end

GDP0 = 11394358246872.6
tspan = (1.0, 59.0)

plot!(1:59,GDP,legend=false)

p = param([80.0,0.60, 800.0, 0.00, 1.0/200])
if false
    prob = ODEProblem(monomial,[GDP0],tspan,p)
else
    prob = ODEProblem(simplemonolinearkoef,[GDP0],tspan,p)
end
function predict_rd() # Our 1-layer neural network
  diffeq_rd(p,prob,Tsit5(),saveat=1.0)
end

function loss_rd() ##L2 norm biases the newer times unfairly
    ##Graph looks better if we minimize relative error squared
    c = 0.0
    a = predict_rd()
    d = 0.0
    for i=1:59

        c += (a(i)[1]/GDP[i]-1)^2
    end
    c + 3 * d
end

data = Iterators.repeated((), 10000)
opt = ADAM(0.000008)


peek = function () #callback function to observe training
    #reduces training speed by a lot
    plot(1:59,GDP,legend=false)
  display(plot!(solve(remake(prob,p=Flux.data(p)),Tsit5(),saveat=1.0),linewidth=0.8))
  plot!(1:59,GDP)
  println("Loss: ",loss_rd())
end

peek()
for i=1:100
  Flux.train!(loss_rd, [p], data, opt)
  peek()
end

plot(1:59,GDP)
display(plot!(solve(remake(prob,p=Flux.data(p)),Tsit5(),saveat=0.1)))
println("Loss: ",loss_rd())

backup = p

p1 = [475.02154720559133,  0.8625507245324309, 426.41136131724187, 0.001, -0.005 ]
# Loss: 0.31527485557904394
p2 = [  320.2478682003448, 0.8874263459675171, 561.8652441836432, 0.001, 0.005   ]
# Loss: 0.31527485557904394
mlk1 = [475.033226455796, 0.8719369097064453, 426.3999415263866, -0.03241787330807177, -0.0004944154462741935, 0.0]
#  Loss: 0.07113143843435651
mqk1 = [ 475.0390522786985, 0.8775913861824332, 426.3940356156704, -0.04243561213029903, -0.0012897258337950205,  1.621522809171344e-5 ]

sqmk1 = [ 474.8501513113645, 0.7036417845990167, 0 , 0, -0.0013254192046426529, 2.147655267911225e-5 ]

plot!(1:59, [1.13944e13, 1.21111e13, 1.28391e13, 1.35778e13, 1.43268e13, 1.50856e13, 1.5854e13, 1.66318e13, 1.74187e13, 1.82147e13, 1.90198e13, 1.98339e13, 2.06571e13, 2.14897e13, 2.23317e13, 2.31835e13, 2.40453e13, 2.49175e13, 2.58006e13, 2.66948e13, 2.76009e13, 2.85193e13, 2.94507e13, 3.03958e13, 3.13552e13, 3.23298e13, 3.33205e13, 3.43281e13, 3.53537e13, 3.63983e13, 3.74629e13, 3.85487e13, 3.96571e13, 4.07895e13, 4.19473e13, 4.31321e13, 4.43458e13, 4.55901e13, 4.6867e13, 4.81784e13, 4.95265e13, 5.09142e13, 5.23445e13, 5.38194e13, 5.53414e13, 5.69134e13, 5.85386e13, 6.02208e13, 6.19642e13, 6.37733e13, 6.56531e13, 6.76091e13, 6.96471e13, 7.17735e13, 7.39948e13, 7.63183e13, 7.87515e13, 8.13023e13, 8.39793e13])
